<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Deyao Zhu</title>

    <meta name="author" content="Deyao Zhu">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Zhu, Deyao Êú±Âæ∑Â∞ß</name>
                                    </p>
                                    <p>Welcome! I am a PhD candidate at <a href="https://cemse.kaust.edu.sa/vcc">KAUST</a>, where I work on <strong>Multimodal Large Language Model</strong>, <strong>Prediction Model</strong>, and <strong>Reinforcement Learning</strong>                                        advised by
                                        <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>. I'm a Hokkien Chinese from Quanzhou.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="tsu.tikgiau@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/resume_deyao.pdf">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=dENNKrsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                        <a href="https://github.com/TsuTikgiau/">GitHub</a> &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/deyao-zhu-205774154/">Linkedin</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="img/photo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/photo.png" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        My research interests lie in AGI. In particular, I am interested in designing multimodal language models that can make decision. This includes Reinforcement Learning, Video & Language Understanding, Planning via Large Language Model, Motion Forecasting,
                                        and other related deep learning topics.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/minigpt4.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2304.04227">
                                        <papertitle>MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu*</strong>,
                                    <a href="https://junchen14.github.io/">Jun Chen*</a>,
                                    <a href="https://xiaoqian-shen.github.io/">Xiaoqian Shen</a>,
                                    <a href="https://xiangli.ac.cn/">Xiang Li</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>Preprint</em>
                                    <br>
                                    <a href="https://arxiv.org/abs/2304.10592">arXiv</a> /
                                    <a href="https://github.com/Vision-CAIR/MiniGPT-4">code</a> /
                                    <a href="https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link">model</a> /
                                    <a href="https://drive.google.com/file/d/1nJXhoEcy3KTExr17I7BXqY5Y9Lx_-n-9/view?usp=share_link">dataset</a> /
                                    <a href="https://minigpt-4.github.io/">website</a> /
                                    <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">demo</a> /
                                    <a href="https://youtu.be/__tftoxpBAw">video</a>
                                    <p></p>
                                    <p>
                                        MiniGPT-4 shows that the secret behind the next-level vision-language-ability of GPT-4 can be simply a more powerful LLM. By aligning open-sourced vision and advanced language models together, MiniGPT-4 reproduces many GPT-4's vision-related demo.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/videochat.gif' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2304.04227">
                                        <papertitle>Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://junchen14.github.io/">Jun Chen</a>,
                                    <strong>Deyao Zhu</strong>,
                                    <a href="https://kilichbek.github.io/webpage/">Kilichbek Haydarov</a>,
                                    <a href="https://xiangli.ac.cn/">Xiang Li</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>Preprint</em>
                                    <br>
                                    <a href="https://arxiv.org/abs/2304.04227">arXiv</a> /
                                    <a href="https://github.com/Vision-CAIR/ChatCaptioner">code</a>
                                    <p></p>
                                    <p>
                                        Video ChatCaptioner creates comprehensive spatiotemporal video descriptions by letting ChatGPT to select the video frame it want to know and ask questions to BLIP-2. ChatGPT at the end summarizes all the information from BLIP-2 as the final video description.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/chatcaptioner.gif' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2303.06594">
                                        <papertitle>ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>,
                                    <a href="https://junchen14.github.io/">Jun Chen</a>,
                                    <a href="https://kilichbek.github.io/webpage/">Kilichbek Haydarov</a>,
                                    <a href="https://xiaoqian-shen.github.io/">Xiaoqian Shen</a>,
                                    <a href="https://cemse.kaust.edu.sa/vcc/people/person/wenxuan-zhang">Wenxuan Zhang</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>Preprint</em>
                                    <br>
                                    <a href="https://arxiv.org/abs/2303.06594">arXiv</a> /
                                    <a href="https://github.com/Vision-CAIR/ChatCaptioner">code</a>
                                    <p></p>
                                    <p>
                                        We discover the powerful questioning ability of modern LLMs. We use it to enrich the image caption of BLIP-2 by prompting ChatGPT to keep asking informative questions to BLIP-2 and summarize the conversation at the end as the final caption.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/zeroseg.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2306.00450">
                                        <papertitle>Exploring Open-Vocabulary Semantic Segmentation without Human Labels</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://junchen14.github.io/">Jun Chen</a>,
                                    <strong>Deyao Zhu</strong>,
                                    <a href="https://guochengqian.github.io/">Guochen Qian</a>,
                                    <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>, Zhicheng Yan, Chenchen Zhu, Fanyi Xiao,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>, Sean Chang Culatana
                                    <br>
                                    <em>Preprint</em>
                                    <br>
                                    <a href="https://arxiv.org/pdf/2306.00450.pdf">arXiv</a>
                                    <p></p>
                                    <p>
                                        ZeroSeg,a novel method that leverages the existing pretrained vision-language(VL) model to train open-vocabulary zero-shot semantic segmentation models
                                    </p>
                                </td>
                            </tr>



                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/afguide.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2301.12876">
                                        <papertitle>Guiding Online Reinforcement Learning with Action-Free Offline Pretraining</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>,
                                    <a href="https://wangyuhuix.github.io/">Yuhui Wang</a>,
                                    <a href="https://people.idsia.ch/~juergen/">J√ºrgen Schmidhuber</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>Preprint</em>
                                    <br>
                                    <a href="https://arxiv.org/abs/2301.12876">arXiv</a> /
                                    <a href="https://github.com/Vision-CAIR/AF-Guide">code</a>
                                    <p></p>
                                    <p>
                                        Extract knowledge from datasets without action labels to help online reinforcement learning by pretraining an Action-Free Decision Transformer to form intrinsic rewards.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/vmg.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2206.04384">
                                        <papertitle>Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>,
                                    <a href="http://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>ICLR, 2023</em>
                                    <br>
                                    <a href="https://openreview.net/forum?id=UYcIheNY9Pf">openreview</a> /
                                    <a href="https://arxiv.org/abs/2206.04384">arXiv</a> /
                                    <a href="https://github.com/TsuTikgiau/ValueMemoryGraph">code</a>
                                    <p></p>
                                    <p>
                                        Applying RL methods on a graph world model instead of the original complex environment simplifies the policy learning.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/social.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2203.03057">
                                        <papertitle>Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.abduallahmohamed.com/home">Abduallah Mohamed</a>,
                                    <strong>Deyao Zhu</strong>, Warren Vu,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a> Christian Claudel,
                                    <br>
                                    <em>ECCV</em>, 2022
                                    <br>
                                    <a href="https://arxiv.org/abs/2203.03057">arXiv</a> /
                                    <a href="https://github.com/abduallahmohamed/Social-Implicit">code</a> /
                                    <a href="https://www.abduallahmohamed.com/social-implicit-amdamv-adefde-demo">demo</a>
                                    <p></p>
                                    <p>A better metric for trajectory prediction that consider the whole prediction distribution.</p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/reltran.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2104.11934">
                                        <papertitle>RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://junchen14.github.io/">Jun Chen</a>, Aniket Agarwal, Sherif Abdelkarim,
                                    <strong>Deyao Zhu</strong>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>CVPR</em>, 2022
                                    <br>
                                    <a href="https://arxiv.org/abs/2104.11934">arXiv</a>/
                                    <a href="https://github.com/Vision-CAIR/RelTransformer">code</a>
                                    <p></p>
                                    <p>Modeling an effective message-passing flow through an attention mechanism can be critical to tackling the compositionality and long-tail challenges in visual relationship recognition.</p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/unlike.png' width="160">
                                    </div>

                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://openreview.net/forum?id=4u25M570Iji">
                                        <papertitle>Motion Forecasting with Unlikelihood Training in Continuous Space</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>,
                                    <a href="https://www.linkedin.com/in/mzahran001/">Mohamed Zahran</a>,
                                    <a href="http://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>,
                                    <a href="http://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>CoRL</em>, 2021 &nbsp
                                    <font color="red"><strong>(Oral Presentation)</strong></font>
                                    <br>
                                    <a href="https://openreview.net/forum?id=4u25M570Iji">openreview</a> /
                                    <a href="https://github.com/Vision-CAIR/UnlikelihoodMotionForecasting">code</a>
                                    <p></p>
                                    <p>Reducing the likelihood of the context-violating predictions directly in the predicted distribution improves the prediction quality. </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/halent.png' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://openreview.net/forum?id=9GBZBPn0Jx">
                                        <papertitle>HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>,
                                    <a href="http://matthewtancik.com/">Mohamed Zahran</a>,
                                    <a href="http://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>,
                                    <a href="https://sites.google.com/site/skywalkeryxc/">Mohamed Elhoseiny</a>
                                    <br>
                                    <em>ICLR</em>, 2021
                                    <br>
                                    <a href="https://openreview.net/forum?id=9GBZBPn0Jx">openreview</a> /
                                    <a href="https://www.youtube.com/watch?v=PEe4fhyvoeQ">video</a>
                                    <p></p>
                                    <p>Hallucinating surrounding vehicles' driving intents helps model predict better.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='img/disent.gif' width="160">
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-33676-9_42">
                                        <papertitle>Learning to Disentangle Latent Physical Factors for Video Prediction</papertitle>
                                    </a>
                                    <br>
                                    <strong>Deyao Zhu</strong>, Marco Munderloh,
                                    <a href="http://www.tnt.uni-hannover.de/staff/rosenhahn/">Bodo Rosenhahn</a>,
                                    <a href="https://is.mpg.de/~jstueckler">J√∂rg St√ºckler</a>
                                    <br>
                                    <em>GCPR</em>, 2019
                                    <br>
                                    <a href="https://openreview.net/forum?id=p9CLg1kBOKg">openreview</a> /
                                    <a href="https://github.com/TsuTikgiau/DisentPhys4VidPredict">code</a> /
                                    <a href="https://www.youtube.com/watch?app=desktop&v=PZ9D4pqhkxs">video</a>
                                    <p></p>
                                    <p>
                                        Reducing the total correlation of the latent feature's dimensions to learn a physically disentangled representation of blocks.</p>
                                </td>
                            </tr>


                        </tbody>
                    </table>


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Misc</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>

                            <tr>
                                <td width="75%" valign="center">
                                    <strong>Third-Place</strong> in <a href="https://aihabitat.org/challenge/2022_rearrange/">Habitat Rearrangement Challenge 2022</a>
                                </td>
                            </tr>

                            <tr>
                                <td width="75%" valign="center">
                                    <strong>Reviewer</strong> in TPAMI, CoRL 2022, ECCV 2022, AAAI 2023, CVPR 2023
                                </td>
                            </tr>
                            <tr>
                                <td width="75%" valign="center">
                                    <strong>Teaching Assistant</strong> in CS283 Deep Generative Model and CS326 Low Resource Deep Learning
                                </td>
                            </tr>



                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <a href="https://jonbarron.info/">Template</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>